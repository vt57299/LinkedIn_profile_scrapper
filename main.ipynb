{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12fcc65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --q Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc3cdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b2e843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading credentials\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('credentials.env')\n",
    "\n",
    "# Get the values of the environment variables\n",
    "username = os.getenv(\"user_name\")\n",
    "password = os.getenv(\"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319d1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Libraries\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "# from undetected_chromedriver import Chrome, ChromeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4215bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape the profiles\n",
    "def scrape_linkedin_profiles(search_query, username, password):\n",
    "    start_time = time.time()       # To check the time throughout the whole process.\n",
    "\n",
    "    try:\n",
    "        # Set up the Chrome driver\n",
    "        driver = webdriver.Chrome()\n",
    "        \n",
    "        # Log in to LinkedIn\n",
    "        driver.get('https://www.linkedin.com/login')\n",
    "\n",
    "        # Find the username and password fields and enter the login credentials\n",
    "        username_field = driver.find_element(By.ID, 'username')\n",
    "        username_field.send_keys(username)\n",
    "\n",
    "        password_field = driver.find_element(By.ID, 'password')\n",
    "        password_field.send_keys(password)\n",
    "        password_field.submit()\n",
    "        \n",
    "        # Handle OTP if required\n",
    "        try:\n",
    "            WebDriverWait(driver, 3).until(EC.visibility_of_element_located((By.ID, 'input__email_verification_pin')))\n",
    "            otp = input(\"Please enter the OTP: \")\n",
    "            otp_field = driver.find_element(By.ID, 'input__email_verification_pin')\n",
    "            otp_field.send_keys(otp)\n",
    "            otp_field.submit()\n",
    "        except TimeoutException:\n",
    "            pass  # Continue without entering OTP if it's not required\n",
    "        \n",
    "        # Wait for the login process to complete\n",
    "        driver.implicitly_wait(3)\n",
    "\n",
    "        # Navigate to the search page\n",
    "        driver.get('https://www.linkedin.com/search/results/people/')\n",
    "\n",
    "        # Enter the search query and submit the search\n",
    "        search_bar = driver.find_element(By.CSS_SELECTOR, '.search-global-typeahead__input')\n",
    "        search_bar.send_keys(search_query)\n",
    "        search_bar.send_keys(Keys.ENTER)\n",
    "\n",
    "        # Wait for the search results to load\n",
    "        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, '.reusable-search__result-container')))\n",
    "\n",
    "        # Get the list of search results\n",
    "        search_results = driver.find_elements(By.CSS_SELECTOR, '.reusable-search__result-container')\n",
    "\n",
    "        # Initialize the CSV file\n",
    "        csv_file = open('profile_data.csv', 'w', newline='', encoding='utf-8')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Name', 'Headline', 'Location', 'Profile URL', 'Connections'])\n",
    "\n",
    "        count = 0\n",
    "        # Scrape data from each profile page\n",
    "        for result in search_results:\n",
    "            try:\n",
    "                name_element = WebDriverWait(result, 4).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a.app-aware-link')))\n",
    "                \n",
    "                # Extract profile URL\n",
    "                profile_url = name_element.get_attribute('href')\n",
    "                \n",
    "                # Click on the profile to open it\n",
    "                ActionChains(driver).key_down(Keys.CONTROL).click(name_element).key_up(Keys.CONTROL).perform()\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "                # Wait for the profile page to load\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'application-outlet')))\n",
    "\n",
    "                # Scrape profile information\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                profile_name_element = soup.find('h1', class_='text-heading-xlarge inline t-24 v-align-middle break-words')\n",
    "                profile_name = profile_name_element.get_text().strip()\n",
    "\n",
    "                headline_element = soup.find('div', class_='text-body-medium break-words')\n",
    "                headline = headline_element.get_text().strip() if headline_element else 'None'\n",
    "\n",
    "                location_element = soup.find('span', class_='text-body-small inline t-black--light break-words')\n",
    "                location = location_element.get_text().strip() if location_element else 'None'\n",
    "\n",
    "                connections_element = soup.find('span', class_='t-bold')\n",
    "                connections = connections_element.get_text().strip() if connections_element else 'None'\n",
    "                \n",
    "                # Save profile information to CSV\n",
    "                csv_writer.writerow([profile_name, headline, location, profile_url, connections])\n",
    "                count += 1\n",
    "\n",
    "            except NoSuchElementException as e:\n",
    "                print(\"Element not found:\", e)\n",
    "                \n",
    "            except TimeoutException as e:\n",
    "                print(\"Timeout:\", e)\n",
    "            \n",
    "            except AttributeError as e:   # used to skip 'LinkedIn Member' profiles.\n",
    "                print(\"Skipping private profile...\")\n",
    "                continue\n",
    "\n",
    "            finally:\n",
    "                # Close the current tab and switch back to the search results tab\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "    finally:\n",
    "        # Close CSV file\n",
    "        csv_file.close()\n",
    "\n",
    "        # Close the webdriver\n",
    "        driver.quit()\n",
    "\n",
    "    # Calculate the time taken\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    print(f\"{count} profiles scrapped successfully!\")\n",
    "    print(f\"Time taken: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f406ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: richie\n",
      "Skipping private profile...\n",
      "9 profiles scrapped successfully!\n",
      "Time taken: 215.57 seconds\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "search_query = input('Enter your search query: ')\n",
    "scrape_linkedin_profiles(search_query,username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239edea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
